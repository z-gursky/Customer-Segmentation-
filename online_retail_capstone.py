# -*- coding: utf-8 -*-
"""Online Retail Capstone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WccmjQZ8zV4GKgdZp4O7LDjnoYfOz7P1
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import datetime
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import MiniBatchKMeans
from sklearn import datasets, metrics
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering
from sklearn.manifold import TSNE
import time

from google.colab import files
data_to_load = files.upload()

import io
online_retail = pd.read_csv(io.BytesIO(data_to_load['online_retail.csv']))
online_retail.shape

"""This data comes from [Kaggle](https://www.kaggle.com/mashlyn/online-retail-ii-uci)

**Data Set Information:** 

This data set contains all transactions between 1/12/2009 - 9/12/2011 for a UK-based online store. This company sells unique all-occasion gift-ware and many customers of the company are wholesalers. 

**Attribute Information:**



*   **Invoice:** Invoice number, if this starts with a 'c', it indicates a cancellation. 
*   **StockCode:** *Product (item) code.*
*   **Description:** *Product (item) name.*
*   **Quantity:** *The number of each product (item) per transaction.*
*   **InvoiceDate:** *Invoice date and time.* 
*   **Customer ID:** *A 5-digit intergral number uniquely assigned to each customer.*
*   **Country:** *Country name where the customer resides.* 


**Business Goal:** 
Segment customer based on RFM so the company can target its customers more efficiently.

*   **Recency (R):** *Number of days since customers last purchase.* 
*   **Frequency (F):** *Number of transactions from the customer.*
*   **Monetary (M):** *Revenue contributed to the company.*

# **Data Cleaning**
"""

online_retail.head()

online_retail.info()

# Which data is missing and how much?
def missing_data(df):
  df = df.isnull().sum()*100/df.isnull().count()
  print(df)
missing_data(online_retail)

def drop_negative_values(df):
  df = df[~(df['Quantity'] < 0)] #Returned items that appear as negative
  return df
online_retail = drop_negative_values(online_retail)
missing_data(online_retail)

def cleaned_data(df):
  df = df.dropna()
  return df
online_retail = cleaned_data(online_retail)
missing_data(online_retail)

"""I have enough data to work with, will drop missing data entirely."""

def transform_column_data(df):
  df["Amount"] = df["Quantity"] * df['Price']
  df["InvoiceDate"] = pd.to_datetime(df["InvoiceDate"],format='%m/%d/%Y %H:%M')
  return df
online_retail = transform_column_data(online_retail)

"""Create a new dataframe with 3 groups:

1) Average revenue contributed by customer
2) Total number of transactions
3) Last transaction made
"""

# create new dataframe and add all purchases together
revenue = online_retail.groupby("Customer ID")['Amount'].sum()
revenue = revenue.reset_index()

# Count unique purchases by Invoice (same invoice #'s represent same day purchases)
transactions = online_retail.groupby("Customer ID")['Invoice'].nunique()
transactions = transactions.reset_index()

#create one dataframe
retail = pd.merge(revenue, transactions, on="Customer ID", how="inner")

# create column including date last shopped
max_date = max(online_retail['InvoiceDate'])
online_retail["recency"] = max_date - online_retail["InvoiceDate"]
online_retail.head()

# Most recent visit
recent = online_retail.groupby("Customer ID")["recency"].min()
recent = recent.reset_index()
# Transform to days only 
recent['recency'] = recent['recency'].dt.days 

retail = pd.merge(retail, recent, on="Customer ID", how="inner")
retail.rename(columns={"Customer ID": "CustomerID", "Amount": "Amount_spent", "Invoice": "Shop_count", "recency": "Most_recent"}, inplace=True)

retail

# correlation between variables
def outlier_visual(df):
  with plt.rc_context({'xtick.color':'white', 'ytick.color':'white', 
                      'axes.labelcolor': 'white'}):
    boxplot = df.boxplot()
outlier_visual(retail)

# Removing (statistical) outliers for Amount
def remove_outliers(df):
  Q1 = df.Amount_spent.quantile(0.10)
  Q3 = df.Amount_spent.quantile(0.90)
  IQR = Q3 - Q1
  df = df[(df.Amount_spent >= Q1 - 1.5*IQR) & (df.Amount_spent <= Q3 + 1.5*IQR)]

  # Removing (statistical) outliers for Recency
  Q1 = df.Most_recent.quantile(0.10)
  Q3 = df.Most_recent.quantile(0.90)
  IQR = Q3 - Q1
  df = df[(df.Most_recent >= Q1 - 1.5*IQR) & (df.Most_recent <= Q3 + 1.5*IQR)]

  # Removing (statistical) outliers for Frequency
  Q1 = df.Shop_count.quantile(0.10)
  Q3 = df.Shop_count.quantile(0.90)
  IQR = Q3 - Q1
  df = df[(df.Shop_count >= Q1 - 1.5*IQR) & (df.Shop_count <= Q3 + 1.5*IQR)]
  return df
retail = remove_outliers(retail)

# correlation between variables
def outlier_visual(df):
  with plt.rc_context({'xtick.color':'white', 'ytick.color':'white', 
                      'axes.labelcolor': 'white'}):
    boxplot = df.boxplot()
outlier_visual(retail)

def scaled_df(data):

  X = retail[["Amount_spent", "Shop_count", "Most_recent"]]

  scaler = StandardScaler()
  X_std = scaler.fit_transform(X)
  return X_std
X_std = scaled_df(retail)

def cluster_number(cluster_method):
  with plt.rc_context({'xtick.color':'white', 'ytick.color':'white', 
                      'axes.labelcolor': 'white'}):
    distortions = []
    K = range(1,10)
    for k in K:
      kmeans_cluster = cluster_method(n_clusters=k)
      kmeans_cluster.fit(X_std)
      distortions.append(kmeans_cluster.inertia_)

    plt.figure(figsize=(16,8))
    plt.plot(K, distortions, 'bx-')
    plt.xlabel('k')
    plt.ylabel('Distortion')
    plt.title('The Elbow Method showing the optimal k', color='white')
    plt.show()
    return distortions
cluster_number(KMeans)

kmeans_cluster = KMeans(n_clusters=3, random_state=123)
# Fit model
kmeans_cluster.fit(X_std)
y_pred = kmeans_cluster.predict(X_std)
# make labels to index coloring
labels = kmeans_cluster.labels_
clusters = pd.concat([retail, pd.DataFrame({'Cluster':labels})], axis=1)

# Defining the mini-batch k-means
def cluster_algorithm(method):
  pca = PCA(n_components=2).fit_transform(X_std)
  cluster = method
  # Fit model
  cluster.fit(X_std)
  
  if method == "Kmeans":
    cluster = cluster.predict(X_std)
  else:
    cluster = cluster.fit_predict(X_std)
  
  plt.figure(figsize=(10,5))
  colours = ["r", "b", "g"]
  for i in range(pca.shape[0]):
      plt.text(pca[i, 0], pca[i, 1], str(cluster[i]),
              color=colours[labels[i]],
              fontdict={'weight': 'bold', 'size': 50}
          )

  plt.xticks([])
  plt.yticks([])
  plt.axis('off')
  plt.show()

kmeans_cluster = cluster_algorithm(KMeans(n_clusters=3, 
                                          random_state=123))

# Defining the agglomerative clustering
agg_cluster = cluster_algorithm(AgglomerativeClustering(linkage='complete', 
                                      affinity='cosine',
                                      n_clusters=3))

# Defining the GMM clustering
from sklearn.mixture import GaussianMixture
gmm_cluster = cluster_algorithm(GaussianMixture(n_components=3, random_state=123))

plt.figure(figsize=(20,10))
dendrogram(linkage(X_std, method='complete'))
plt.show()

def adjusted_rand_index_score(method):
  pca = PCA(n_components=2).fit_transform(X_std)
  cluster = method
  # Fit model
  cluster.fit(X_std)
  
  if method == "Kmeans":
    cluster = cluster.predict(X_std)
  else:
    cluster = cluster.fit_predict(X_std)

  return cluster
ari_kmeans = adjusted_rand_index_score(KMeans(n_clusters=3, 
                                          random_state=123))
ari_agg = adjusted_rand_index_score(AgglomerativeClustering(linkage='complete', 
                                      affinity='cosine',
                                      n_clusters=3))
ari_gmm = adjusted_rand_index_score(GaussianMixture(n_components=3, random_state=123))

print("Adjusted Rand Index of the Kmeans Clustering solution: {}"
      .format(metrics.adjusted_rand_score(labels, ari_kmeans)))
print("The silhoutte score of the Kmeans Clustering solution: {}"
      .format(metrics.silhouette_score(X_std, ari_kmeans, metric='euclidean')))
print("---------------------------------------------------------")
print("Adjusted Rand Index of the Agglomerative Clustering solution: {}"
      .format(metrics.adjusted_rand_score(labels, ari_agg)))
print("The silhoutte score of the Agglomerative Clustering solution: {}"
      .format(metrics.silhouette_score(X_std, ari_agg, metric='euclidean')))
print("----------------------------------------------------------")
print("Adjusted Rand Index of the Gaussian Mixture clustering solution: {}"
      .format(metrics.adjusted_rand_score(labels, ari_gmm)))
print("The silhoutte score of the Gaussian Mixture clustering solution: {}"
      .format(metrics.silhouette_score(X_std, ari_gmm, metric='euclidean')))

retail["Cluster"] = labels
with plt.rc_context({'xtick.color':'white', 'ytick.color':'white', 
                     'axes.labelcolor': 'white'}):
  plt.figure(figsize=(15,8))
  plt.subplot(1,3,1)
  sns.boxplot(x='Cluster', y='Amount_spent', data=retail)

  plt.subplot(1,3,2)
  sns.boxplot(x='Cluster', y='Shop_count', data=retail)

  plt.subplot(1,3,3)
  sns.boxplot(x='Cluster', y='Most_recent', data=retail)

"""Interpertations of Clusters:



*   **Cluster 0:**


> Amount spent is Low-Medium without frequently shopping at this store. They have returned within the past few months. 


*   **Cluster 1:**


> This cluster spent the least amount of money, shopped the most infrequent and have not been back 1-2 years. 



*   **Cluster 2:**


> Highest amount spent with returning to the store most frequently and most recent.

We do not have exsisting labels so I will use Silhoutte score.


*   Values of silhoutte score range between -1 to 1. 


*   Scores closer to 1 indicate the data point is very similar to other points in the cluster.


*   Scores closer to -1 indicate that the data point is not similar to the data points in its cluster. 

It appears that the Kmeans Clustering Algorithm used produced the best silhoutte score. This will give us clusters of datapoints that are closer to other datapoints in the cluster than they are to datapoints in other clusters. From the visualizations above it appears cluster 1 has the least importance and cluster 2 has the most importance with cluster 0 somewhere in between from a business point of view.
"""